
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Tech Deep Dives and Tutorials">
      
      
        <meta name="author" content="Abraham Olu">
      
      
        <link rel="canonical" href="https://abeowlu.github.io/article8-netwrking/">
      
      
        <link rel="prev" href="..">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.49">
    
    
      
        <title>About - Tech Deep Dives and Tutorials</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.6f8fc17f.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#manual-setup-of-kubernetes-services-using-iptables" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Tech Deep Dives and Tutorials" class="md-header__button md-logo" aria-label="Tech Deep Dives and Tutorials" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="m290.8 48.6 78.4 29.7-81.2 31.2-81.2-31.2 78.4-29.7c1.8-.7 3.8-.7 5.7 0zM136 92.5v112.2c-1.3.4-2.6.8-3.9 1.3l-96 36.4C14.4 250.6 0 271.5 0 294.7v119.2c0 22.2 13.1 42.3 33.5 51.3l96 42.2c14.4 6.3 30.7 6.3 45.1 0L288 457.5l113.5 49.9c14.4 6.3 30.7 6.3 45.1 0l96-42.2c20.3-8.9 33.5-29.1 33.5-51.3V294.8c0-23.3-14.4-44.1-36.1-52.4L444 206c-1.3-.5-2.6-.9-3.9-1.3V92.5c0-23.3-14.4-44.1-36.1-52.4L308 3.7c-12.8-4.8-26.9-4.8-39.7 0l-96 36.4C150.4 48.4 136 69.3 136 92.5m256 118.1-82.4 31.2v-89.2L392 121zm-237.2 40.3 78.4 29.7-81.2 31.1-81.2-31.1 78.4-29.7c1.8-.7 3.8-.7 5.7 0zm18.8 204.4V354.8l82.4-31.6v95.9zm247.6-204.4c1.8-.7 3.8-.7 5.7 0l78.4 29.7-81.3 31.1-81.2-31.1zm102 170.3-77.6 34.1V354.8l82.4-31.6v90.7c0 3.2-1.9 6-4.8 7.3"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Tech Deep Dives and Tutorials
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              About
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Tech Deep Dives and Tutorials" class="md-nav__button md-logo" aria-label="Tech Deep Dives and Tutorials" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="m290.8 48.6 78.4 29.7-81.2 31.2-81.2-31.2 78.4-29.7c1.8-.7 3.8-.7 5.7 0zM136 92.5v112.2c-1.3.4-2.6.8-3.9 1.3l-96 36.4C14.4 250.6 0 271.5 0 294.7v119.2c0 22.2 13.1 42.3 33.5 51.3l96 42.2c14.4 6.3 30.7 6.3 45.1 0L288 457.5l113.5 49.9c14.4 6.3 30.7 6.3 45.1 0l96-42.2c20.3-8.9 33.5-29.1 33.5-51.3V294.8c0-23.3-14.4-44.1-36.1-52.4L444 206c-1.3-.5-2.6-.9-3.9-1.3V92.5c0-23.3-14.4-44.1-36.1-52.4L308 3.7c-12.8-4.8-26.9-4.8-39.7 0l-96 36.4C150.4 48.4 136 69.3 136 92.5m256 118.1-82.4 31.2v-89.2L392 121zm-237.2 40.3 78.4 29.7-81.2 31.1-81.2-31.1 78.4-29.7c1.8-.7 3.8-.7 5.7 0zm18.8 204.4V354.8l82.4-31.6v95.9zm247.6-204.4c1.8-.7 3.8-.7 5.7 0l78.4 29.7-81.3 31.1-81.2-31.1zm102 170.3-77.6 34.1V354.8l82.4-31.6v90.7c0 3.2-1.9 6-4.8 7.3"/></svg>

    </a>
    Tech Deep Dives and Tutorials
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    About
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    About
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#implementation" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Implementation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create-the-container-network" class="md-nav__link">
    <span class="md-ellipsis">
      Create the Container Network
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-the-pod-container" class="md-nav__link">
    <span class="md-ellipsis">
      Create the Pod Container
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wiring-up-the-pod-network" class="md-nav__link">
    <span class="md-ellipsis">
      Wiring Up The Pod Network
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#glossary" class="md-nav__link">
    <span class="md-ellipsis">
      Glossary
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Glossary">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#non-eks-ec2-compute-amis" class="md-nav__link">
    <span class="md-ellipsis">
      Non-EKS EC2 Compute AMIs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#implementation" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Implementation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create-the-container-network" class="md-nav__link">
    <span class="md-ellipsis">
      Create the Container Network
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-the-pod-container" class="md-nav__link">
    <span class="md-ellipsis">
      Create the Pod Container
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wiring-up-the-pod-network" class="md-nav__link">
    <span class="md-ellipsis">
      Wiring Up The Pod Network
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#glossary" class="md-nav__link">
    <span class="md-ellipsis">
      Glossary
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Glossary">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#non-eks-ec2-compute-amis" class="md-nav__link">
    <span class="md-ellipsis">
      Non-EKS EC2 Compute AMIs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="manual-setup-of-kubernetes-services-using-iptables">Manual Setup of Kubernetes Services using IPtables</h1>
<p>This post inroduces a series of articles introducing a low level look into how container networking can be set up in a Kubernetes cluster.</p>
<p>The Kubernetes project <a href="https://www.cni.dev/docs/">defines a CNI standard</a> that all cluster CNI (container network interface) adhere to. However, asides from exposing the expected plugin endpoint for Kubernetes container orchestration, exact details on how each CNI implements container networking is left up to the designers of each CNI. Meaning exact implementation of CNI varies from one plugin to the next.</p>
<p>We attempt a hands-on implementation of how a CNI would set up container networking in a cluster, ignoring the exposing the CNI endpoints for kubernetes for now. Instead we focus on setting up a container network with;</p>
<ul>
<li>iptables used to configure network routes for pod-to-pod, and pod-to-service and pod-to-external-endpoint connections,</li>
<li>Linux networking to set up isolated pod(container) network namespaces, </li>
<li>ipvs and nftable implementation, replacing iptables - to reduce the number of iptables rules required for pod communication,</li>
<li><em>and round off with exposing the CNI spec endpoint</em> </li>
<li>possibly some troubleshooting tips for <a href="https://github.com/aws/amazon-vpc-cni-k8s">aws-vpc-cni</a>(CNI)
~~liveness/readiness probe errors, and component failures connecting to <code>kubernetes.svc</code> API endpoint within cluster.~~</li>
</ul>
<h3 id="implementation">Implementation</h3>
<p>We start of our discussion in this post by creating a manual network namespace for two containers, while ensuring to point out what part of the pod network creation process would be normally be handled by Kubelet, CRI, CNI and kube-proxy. The network configurations can be implemented on any Linux environment, or even an existing cluster Linux worker node with running workload. The custom container network we create are isolated, and should ordinarily not interfere with any existing infrastructure It may also provide some insight to be able to contrast our manual implementation with CNI network on existing worker nodes.</p>
<p>Some simplification we introduce in creating a pod sandbox eliminates certain Kubernetes complexities during pod creation such as;</p>
<ul>
<li>kubelet creating pods' cgroup under /run/systemd/system/kubepods.slice hierarchy, burstable, best-effort, etc., instead we use the default user slice for the containers</li>
<li>kuberntes container runtime interface (CRI) creating a sandbox for pod under /var/lib/containerd/io.containerd.runtime.v2.task/default/, instead we use the "default" containerd namespace</li>
</ul>
<p>we however, do the container network interface (CNI) tasks of creating a network namespace (net-ns), managing IP, and wiring up the network, similar to how they are done by Kubernetes CNI applicatinos. This is a very CNI-dependent process, but the networking expectations remain the same. So let's dive right in shall we.</p>
<h4 id="create-the-container-network">Create the Container Network</h4>
<p>Let's start with a few handy commands to take a look at the current network namespaces(net-ns) on the computer/worker-node, and the network interfaces. We will run these commands often as we progress to see the changes configured. It is possible to already have net namespaces(net-ns) built by a cluster CNI if running on an existing worker node.</p>
<pre><code class="language-sh">sudo ip netns
sudo ip link show
</code></pre>
<ol>
<li>We create two new network namespaces of our own for the pods, <code>pod-1</code> and <code>pod-2</code>. We will call these namespaces <code>veth-ns-test-pod-1</code> and <code>veth-ns-test-pod-2</code>. Nomenclature is to indicate we are using kernel virtual ethernet devices, they can be named differently. Run the handy check commands above after making changes to see the configuration changed on the system.</li>
</ol>
<pre><code class="language-sh">sudo ip netns add veth-ns-test-pod-1
sudo ip netns add veth-ns-test-pod-2
</code></pre>
<ol start="2">
<li>Next, we create the Linux virtual ethernet network devices used by the containers in the network namespaces we just created, and at the same time the peer for these veth interfaces.</li>
</ol>
<pre><code class="language-sh">sudo ip link add veth-pod-1 type veth peer name veth-peer-pod-1
sudo ip link add veth-pod-2 type veth peer name veth-peer-pod-2
</code></pre>
<ol start="3">
<li>In no particular order, we need to move the <code>veth-pod-1</code> and <code>veth-pod-2</code> devices into the network namespaces, <code>veth-ns-test-pod-1</code> and <code>veth-ns-test-pod-2</code>, respectively. And we leave the peer of each veth in the host network namespace for now.</li>
</ol>
<p>|   a. again run the handy check after the configuration below, and <code>veth-pod-1@veth-peer-pod-1</code> and <code>veth-pod-2@veth-peer-pod-2</code> interfaces should no longer be listed in the host's net-ns, while the peers are. The veth interfaces would have been moved into distinct net-ns.</p>
<pre><code>```sh
sudo ip link set dev veth-pod-1 netns veth-ns-test-pod-1
sudo ip link set dev veth-pod-2 netns veth-ns-test-pod-2
```
</code></pre>
<p>|    b. To see the veth devices in their respective namespaces, run;</p>
<pre><code>```sh
sudo ip netns exec veth-ns-test-pod-1 ip link show
sudo ip netns exec veth-ns-test-pod-2 ip link show
```
</code></pre>
<ol start="4">
<li>Notice in the command above that the Loopback interface in the custom pod network namespaces are currently DOWN. We can turn this UP in this step to allow basic (127.0.0.1)loopback network connections. For example, processes(containers) in our network namespaces require this netowrk config in order to perform a self-health check on localhost:8080/healthz</li>
</ol>
<pre><code class="language-sh">sudo ip netns exec veth-ns-test-pod-1 ip link set dev lo up
sudo ip netns exec veth-ns-test-pod-2 ip link set dev lo up
</code></pre>
<ol start="5">
<li>We also note that the veth interfaces moved into the custom network namespaces, and the veth peers left in the host network namespace are also DOWN, lets set all these up;</li>
</ol>
<pre><code class="language-sh">sudo ip link set dev veth-peer-pod-1 up
sudo ip link set dev veth-peer-pod-2 up

sudo ip netns exec veth-ns-test-pod-1 ip link set dev veth-pod-1 up
sudo ip netns exec veth-ns-test-pod-2 ip link set dev veth-pod-2 up
</code></pre>
<hr />
<h4 id="create-the-pod-container">Create the Pod Container</h4>
<p>At this point, we have a rough pod network namespace setup. We can rollout test containers into pod sandboxes using these net-ns and poke around inside the sandbox. This should reveal the current networking limitations, as the setup done by most CNIs is complete, except we haven’t wired up the network with an assigned IP yet, or routes for pod-to-pod or external endpoints connection etc.</p>
<ol start="6">
<li>We create two container sandboxes in a similar directory where containerd (CRI) creates Kubernetes pods' sandboxes. You can poke around this directory if following this on an existing kubernetes cluster worker node and would like to see running pods' container, if there are any.</li>
</ol>
<pre><code class="language-sh">sudo mkdir -p /run/containerd/pod-1/
sudo mkdir -p /run/containerd/pod-2/
</code></pre>
<p>|   a. we are going to pull an nginx container image as the test base image, and create a runc container spec -  a json configuration manifest specifying how runc builds a container. This is similar to kubelet invoking the CRI specified in its own config, which will pull and validate the pod's container images, and then uses a runtime such as runc to start the container.</p>
<pre><code class="language-sh">sudo ctr image pull docker.io/nginxinc/nginx-unprivileged:latest &amp;&amp; \
sudo ctr run --detach docker.io/nginxinc/nginx-unprivileged:latest default-nginx

sudo cp -r /run/containerd/io.containerd.runtime.v2.task/default/default-nginx/rootfs /run/containerd/pod-1/
sudo cp -r /run/containerd/io.containerd.runtime.v2.task/default/default-nginx/rootfs /run/containerd/pod-2/

cd /run/containerd/pod-2/ &amp;&amp; sudo runc spec 
cd /run/containerd/pod-1/ &amp;&amp; sudo runc spec
</code></pre>
<div class="admonition insight">
<p class="admonition-title">Insight</p>
<p>Note: you can always check on what config has changed in the container spec at any point by making a backup now for later comparisons</p>
<p><code>sudo cp config.json config.json.bkp &amp;&amp;
sudo diff -y config.json.bkp config.json</code></p>
</div>
<p>|   b. next, we update the container spec with the manually created network namespaces for both pod-1 and pod-2, and start the container using runc. This is exactly how containerd starts containers for kubelet as well. The network namespaces we created are located by default in <code>/var/run/netns</code></p>
<pre><code class="language-sh">echo $(jq '(.linux.namespaces[] | select(.type == &quot;network&quot;)) += \
{&quot;path&quot;: &quot;/var/run/netns/veth-ns-test-pod-1&quot;}' /run/containerd/pod-1/config.json)  \
| sudo tee -i /run/containerd/pod-1/config.json

echo $(jq '(.linux.namespaces[] | select(.type == &quot;network&quot;)) += \
{&quot;path&quot;: &quot;/var/run/netns/veth-ns-test-pod-2&quot;}' /run/containerd/pod-2/config.json) \
| sudo tee -i /run/containerd/pod-2/config.json
</code></pre>
<p>|   c. we should also set the default nginx startup instructions to ensure the application starts correctly. This is similar to pod manifest <code>.spec.containers[].command</code> in kubernetes. Also, allow container root access (not strictly required for the particular nginx image we selected), and add some Linux cspabilities (CAPs), these are necessary for nginx servers to start up with required network capabilities;</p>
<pre><code class="language-sh">sudo echo $(jq '.process.capabilities.bounding += [ &quot;CAP_CHOWN&quot;, &quot;CAP_SETGID&quot;, &quot;CAP_SETUID&quot; ] \
| .process.capabilities.permitted += [ &quot;CAP_CHOWN&quot;, &quot;CAP_SETGID&quot;, &quot;CAP_SETUID&quot; ] \
| .root.readonly = false | .process.args = [ &quot;/docker-entrypoint.sh&quot;, &quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot; ]' /run/containerd/pod-1/config.json)  \
| sudo jq '.' | sudo tee -i /run/containerd/pod-1/config.json

sudo echo $(jq '.process.capabilities.bounding += [ &quot;CAP_CHOWN&quot;, &quot;CAP_SETGID&quot;, &quot;CAP_SETUID&quot; ] \
| .process.capabilities.permitted += [ &quot;CAP_CHOWN&quot;, &quot;CAP_SETGID&quot;, &quot;CAP_SETUID&quot; ] \
| .root.readonly = false | .process.args = [ &quot;/docker-entrypoint.sh&quot;, &quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot; ]' /run/containerd/pod-2/config.json) \
| sudo jq '.' | sudo tee -i /run/containerd/pod-2/config.json
</code></pre>
<p>|   d. Next, we now run the containers. The way we invoke <a href="https://github.com/opencontainers/runc/blob/main/docs/terminals.md">runc without providing custom file descriptors</a> requires running in foreground mode and use the current shell's primary file descriptors (fds), stdin, stdout and stderr. So we create 2 new terminal sessions to start the 2 containers.</p>
<pre><code class="language-sh">Terminal 1: cd /run/containerd/pod-1/ &amp;&amp; sudo runc run pod-1
Terminal 2: cd /run/containerd/pod-2/ &amp;&amp; sudo runc run pod-1
Terminal 0: sudo runc list
</code></pre>
<div class="admonition info">
<p class="admonition-title">Runc Terminal Handling</p>
<p>In order to keep this discussion streamlined, we are going to let runc capture for the container process the calling shell's fds  - in other words, our current terminal will be captured as stdin, stdout and stderr for the container created. The popular kubernetes CRIs, containerd, CRI-O, etc. manage container runtime by implementing a wrapper around runc and providing a socket file descriptor that runc sets the container up with then exits - this is what implements functionality such as <code>kubectl logs</code> and <code>kubectl exec -it</code> as well.</p>
</div>
<p>|   a. From the default terminal, we can poke arround the pod environment - like check the /proc/net directory setup in the containers. However, first, we should retrieve the pid of the created containers. The PID can be used to enter a container process' namespace, with the <code>nsenter -n -t &lt;PID&gt;</code> command. In this mode runc is running as a parent process to the container, there is no way to exec/attach to the running (*pod)container from the terminal runc has used. For the simplicity of this post, we are simply leaving the terminals hijacked by runc and using another terminal to exec/attach to the container(s).</p>
<pre><code class="language-sh">sudo runc list
sudo nsenter -n -t &lt;PID&gt; curl localhost:8080
sudo nsenter -n -t &lt;PID&gt; curl github.io
sudo nsenter -n -t &lt;PID&gt; cat /proc/net/route
</code></pre>
<p>And to poke around the routes in the pod network use <code>cat /proc/net/route</code> or <code>ip route</code> commands. The <code>localhost</code> connection succeeds from the command above, while the connection to an external <code>github.io</code> endpoint fails with <code>Could not resolve host github.io</code> - as there are no routes besides the one for loopback configured by default. So, lets us finish wiring up the pod networking, shall we?</p>
<h4 id="wiring-up-the-pod-network">Wiring Up The Pod Network</h4>
<ol start="7">
<li>we continue the setup by assigning IPs to both pod-1 and pod-2, from arbitrary private CIDR, 100.64.0.0/16, with minimal chance of collisions with your machine's private network. There is still a chance of collision if your network is using the 100.64.0.0/16 CIDR we chose, you should choose another <a href="https://datatracker.ietf.org/doc/html/rfc1918">private IP range</a></li>
</ol>
<p>Assigning IP is the functionality of a CNI's IPAM in kubernetes cluster network. Note that we are using a virtual pod IP range, like a lot of CNIs, while some others, for example <a href="https://github.com/aws/amazon-vpc-cni-k8s">amazon VP CNI</a>, uses real private IPs from a VPC for a more resource-expensive approach.</p>
<p>And for even further simplicity, we will be using the <code>ip netns</code> command rather than <code>nsenter</code> used prior, to attach to a container's network namespace. We can avoid having to pick up the started container PIDs this way moving forward</p>
<pre><code class="language-sh">sudo ip netns exec veth-ns-test-pod-1 ip addr add 100.64.0.9/24 dev veth-pod-1
sudo ip netns exec veth-ns-test-pod-2 ip addr add 100.64.0.10/24 dev veth-pod-2
</code></pre>
<ol start="8">
<li>we create the pod-to-pod connectivity next using a bridge device. Several CNIs implement different Linux networking device for setting up pod-to-pod connection;</li>
</ol>
<ul>
<li>amazon VPC CNI uses a dummy interface for pod-to-pod routing, real VPC IPs are used, so in-VPC and external networking routes setup is not required</li>
<li>calico/weave uses a tunnel device for pod-to-pod connection, virtual IPs are used, which can be IP resource cheap but expensive routing and connecting to external endpoints</li>
</ul>
<p>|   a. we bring up a Linux network bridge device for the pod-to-pod networking</p>
<pre><code class="language-sh">sudo ip link add custom-cni type bridge 
</code></pre>
<p>|   b. most CNIs use the default gateway address of 169.254.1.1 in pod namespaces. However, for clarity we use the 100.64.0.1 of the pod virtual IP CIDR range</p>
<pre><code class="language-sh">sudo ip addr add 100.64.0.1/24 dev custom-cni
sudo ip link set custom-cni up
</code></pre>
<p>|   c. we plug the veth peer of the pods net interfaces, that are in the host network namespace into the custom-cni bridge device. And while at it, we should check the routes for these veth peer devices on the host net-ns, run the <code>ip route</code> command. Notice the Linux network manager has wired the interfaces routes, also notice veth-peer in host net-ns has been set to be promiscuous (<code>ip -details link show veth-peer-pod-1</code>)</p>
<pre><code class="language-sh">sudo ip link set dev veth-peer-pod-1 master custom-cni
sudo ip link set dev veth-peer-pod-2 master custom-cni
</code></pre>
<p>|    d. lets add a default route to send traffic through the bridge, </p>
<pre><code class="language-sh">sudo ip netns exec veth-ns-test-pod-1 ip route add default via 100.64.0.1
sudo ip netns exec veth-ns-test-pod-2 ip route add default via 100.64.0.1
</code></pre>
<p>|   e. if both containers are not already running from a prior step, start them now. And in a new terminal, we can test connectivity from one pod to the second pod, and itself using the assigned private IP</p>
<pre><code class="language-sh">sudo ip netns exec veth-ns-test-pod-1 curl -ikv 100.64.0.9:8080
sudo ip netns exec veth-ns-test-pod-1 curl -ikv 100.64.0.10:8080
sudo ip netns exec veth-ns-test-pod-1 curl -ikv github.io
</code></pre>
<p>The pod network connection to itself and connection from pod-1 to pod-2 and vice-versa, using assigned IPs succeed, but external connection to <code>github.io</code> continues to fail. Some familiarity with <a href="http://www.faqs.org/docs/iptables/traversingoftables.html">iptables may be appropriate</a>, but we will highlight iptables rules used here enough for our discussion.</p>
<p>Why does pod-to-pod connections succeed after setting up the bridge? Well, in a pod-to-pod connection on the same host, packets are flowing from one Linux process to another, and not egressing the system through any network interface. The bridge device we created loads with netfilter call mode enabled by default, i.e., kernel iptable is allowed to route traversing packets. And from the perspective of the iptable traversal, packet flow is local to the computer system;</p>
<p><img alt="Pod-1-to-Pod-2" src="https://github.com/abeowlu/kube-journal/blob/docs/docs/assets/ip5.drawio.png?raw=true" /></p>
<div class="admonition warning">
<p class="admonition-title">NB: Troubleshooting Issues</p>
<p>if you have unexpected connection issues at this point. You may not be using a cluster workernode, which would have some necessary network system configurations pre-implemented. See <a href="../network-troubleshooting/">troubleshootoing article section creating bridge with net.bridge.bridge-nf-call-iptables</a>. And check the troubleshooting tips in the article to get grease under your finger nails. </p>
</div>
<hr />
<p>The last CNI task we perform is set up pod-to-external destination network routes. Then we manually implement the kube-proxy component's function of creating custom virtual services, to expose our custom pods, to round off this discussion. We do this by completing the setup of the custom network path as in the image below on the compute node. At any point, you can perform the actions in <code>Troubleshooting tip</code> to analyze captured packets to see its flow;</p>
<p><img alt="image" src="https://github.com/abeowlu/kube-journal/blob/docs/docs/assets/ip4.drawio.png?raw=trueQ" /></p>
<ol start="9">
<li>we set up iptables rule to allow communication from our cluster custom-cni bridge interface on a network range of 100.64.0.0/16 to connect externally through our node's network interface, and whatever CIDR range it is on. From the perspective of iptable traversal, packet flow would look like;</li>
</ol>
<p><img alt="pod-to-external" src="https://github.com/abeowlu/kube-journal/blob/docs/docs/assets/ip1.drawio.png?raw=true" /></p>
<ul>
<li>the switch from our custom-cni bridge interface to the node's primary interface in ROUTING DECISIONS is performed by the kernel networking</li>
<li><code>sudo iptables -L OUTPUT -t mangle &amp;&amp; sudo iptables -L POSTROUTING -t mangle</code> are open chains allowing free packet flow by default, as are the others</li>
<li>the packets from the pods are egressing into the host network using the private virtual IPs we assigned - 100.64.0.9 and 100.64.0.10. Some networks will reject/drop such packets with unknown source IP address as martian packets, or by firewall src/dst checks, for security reasons. We need to SNAT pod traffic to external destinations to avoid this. There are a number of ways to achieve this SNAT, with simplicity as our main consideration - we will implement it in the last chain in the image above, nat-POSTROUTING;</li>
</ul>
<p>|   a. retrieve the primary private IP of your worker node</p>
<pre><code class="language-sh">nodeIP=$(sudo ip -details -j addr show | jq '.[].addr_info[] | select(.label == &quot;eth0&quot;) | .local') &amp;&amp; localIP=$(sudo echo ${nodeIP} | tr -d '&quot;')
</code></pre>
<p>|   b. we insert a rule in the nat-POSTROUTING chain, matching packet from the pod CIDR, and tracking connections to external destination, i.e. not local, and SNAT the packet - change packet header source IP address from the pod IP to the host's primary IP. </p>
<pre><code class="language-sh">sudo iptables -I POSTROUTING -t nat -s 100.64.0.0/16 -m addrtype ! --dst-type LOCAL -m conntrack --ctstate NEW,ESTABLISHED -j SNAT --to-source ${localIP}
</code></pre>
<p>Test: <code>sudo ip netns exec veth-ns-test-pod-1 curl -ikv github.io</code></p>
<ol start="10">
<li>Lastly, we use iptables to expose the pod-1, 100.64.0.9 and pod-2, 100.64.0.10, behind virtual service IPs which we will pick from an arbitraru range of 198.19.0.0/16, with minimal collision chance. We will stick close to how kube-proxy sets up a similar clusterIP service type. First, the iptable traversal from pod-1 to service-2 delivered to pod-2 looks like this;</li>
</ol>
<p><img alt="cluster-IP-flow" src="https://raw.githubusercontent.com/AbeOwlu/kube-journal/refs/heads/docs/docs/assets/ip2.drawio.png?token=GHSAT0AAAAAACR2TFHY7UMUQJPVXJDVXFN4Z2BDZYQ" /></p>
<p>|   a. create a custom network service chains, CUSTOM-CLUSTER-SERVICES, CUSTOM-SERVICES-1 and CUSTOM-SERVICES-2 in the nat table, synonymous to the KUBE-SERVICES chain created by kube-proxy which is used for clear rules management. Then, create the required service endpoint (SEP) chain for service-to-pod routing, also in the nat table;</p>
<pre><code class="language-sh">sudo iptables -t nat -N CUSTOM-CLUSTER-SERVICES
sudo iptables -t nat -N CUSTOM-SERVICES-1
sudo iptables -t nat -N CUSTOM-SERVICES-2
sudo iptables -t nat -N CUSTOM-SEP-1
sudo iptables -t nat -N CUSTOM-SEP-2
</code></pre>
<p>|   b. importantly, we inform the main iptables nat-PREROUTING and nat-OUTPUT chains to look through our nat-CUSTOM-CLUSTER-SERVICES chain during packet processing. If you don't add the nat-OUTPUT rule here, connection from the host to service IP 198.19.0.9:8080 fails, but pod-x-to-service-y succeeds, try to understand what happened to the node's packets to the pod, use the troubleshooting tip and iptables flow diagram above.</p>
<pre><code class="language-sh">sudo iptables -I PREROUTING -t nat -m comment --comment &quot;custom-cluster-services&quot; -j CUSTOM-CLUSTER-SERVICES
sudo iptables -I OUTPUT -t nat -m comment --comment &quot;custom-cluster-services&quot; -j CUSTOM-CLUSTER-SERVICES
</code></pre>
<p>|   c. next, we configure CUSTOM-CLUSTER-SERVICES to look through CUSTOM-SERVICES-1 and CUSTOM-SERVICES-2 chains, we can restrict the network protocols, e.g., tcp(6), udp(17), etc., that the service endpoint accepts in this chain, like kube-proxy, or in any of the later chains. But we leave it open to all protocols for simplicity.</p>
<pre><code class="language-sh">sudo iptables -A CUSTOM-CLUSTER-SERVICES -t nat -d 198.19.0.9 -m comment --comment &quot;Rule to service-1 -&gt; 198.19.0.9&quot; -j CUSTOM-SERVICES-1
sudo iptables -A CUSTOM-CLUSTER-SERVICES -t nat -d 198.19.0.10 -m comment --comment &quot;Rule to service-2 -&gt; 198.19.0.10&quot; -j CUSTOM-SERVICES-2
</code></pre>
<p>|   d. configure the iptables rule in CUSTOM-SERVICES-1 and CUSTOM-SERVICES-2 to look through the service endpoint (SEP) chains, CUSTOM-SEP-1 and CUSTOM-SEP-2, this is another rule that could be eliminated from the stack, but kube-proxy uses it for readability and rule management, and we are sticking with kube-proxy implementation, so;</p>
<pre><code class="language-sh">sudo iptables -A CUSTOM-SERVICES-1 -t nat -m comment --comment &quot;Rule to service-1-endpoionts&quot; -j CUSTOM-SEP-1  
sudo iptables -A CUSTOM-SERVICES-2 -t nat -m comment --comment &quot;Rule to service-2-endpoionts&quot; -j CUSTOM-SEP-2 
</code></pre>
<p>|   e. for the virtual service IP functionality - sending all traffic received at service IP to endpoint pod IP - configure the following rules in the CUSTOM-SEP-X chains for <code>198.19.0.9 to 100.64.0.9</code> and <code>198.19.0.10 to 100.64.0.10</code></p>
<pre><code class="language-sh">sudo iptables -A CUSTOM-SEP-1 -t nat -p tcp -j DNAT --to-destination 100.64.0.9:8080
sudo iptables -A CUSTOM-SEP-2 -t nat -p tcp -j DNAT --to-destination 100.64.0.10:8080
</code></pre>
<p>|   f. test connectivity in all directions, from the node to the service IPs, from pod-1 to service-2 IP, from pod-1 to service-1 IP</p>
<pre><code class="language-sh">curl -ikv 198.19.0.9:8080
curl -ikv 198.19.0.10:8080
sudo ip netns exec veth-ns-test-pod-1 curl 198.19.0.10:8080
...
</code></pre>
<p>One last, thing for the sake of completeness. Note that pod-1 and pod-2 are unable to connect to itself using the service IP exposing them. Test connection from a pod-1 through service-1 IP exposing it, and for pod-2;</p>
<pre><code class="language-sh">sudo ip netns exec veth-ns-test-pod-2  curl -ikv 198.19.0.10:8080
</code></pre>
<p><em><code>Troubleshooting tip</code></em>: You can check the troubleshooting article in this series of discussion for in-depth look. To understand what is occurring to our packets, here, however, let us check the flow, open 3 new terminals</p>
<pre><code class="language-sh">terminal 1: `sudo tcpdump -vvv -n host 198.19.0.10 -i custom-cni`
terminal 2: `sudo ip netns exec veth-ns-test-pod-2 tcpdump -vvv -n host 198.19.0.10 -i any`
terminal 3: `sudo ip netns exec veth-ns-test-pod-2 curl -ikv 198.19.0.10:8080`
</code></pre>
<p>In terminal 1, note that pod-2 is sending SYN packets to the <code>198.19.0.10.webcache</code> endpoint, that are never ACK'd. In terminal 2, the custom-cni bridge is discarding the packet that should be sent from <code>198.19.0.10</code> back to the originating pod-2. This issue is occurring because the bridge does not allow hairpin traffic by default. To correct this, check the current bridge interface configuration and enable promiscuous mode with;</p>
<pre><code class="language-sh">sudo ip -details link show custom-cni
sudo ip link set custom-cni promisc on
</code></pre>
<p>At this point, we have manually created 2 pods running nginx containers and set up a full pod networking on a compute/worker node. Implementing the specification of the OCI CNI requirements](https://github.com/containernetworking/cni) manually, to better understand how cluster networks. To top it off, we have exposed these pods behind our own service IPs within the cluster. In the next discussion, we expose these pods publicly, and look into the benefits of using the newer IPVS mode of kube-proxy over iptables rules management.</p>
<p>Finally, if desired, you can clean up all the implementation above;</p>
<pre><code>##Stopping the containers
sudo runc kill pod-1 KILL 

## Deleting the chains
sudo iptables -t nat -D &lt;$CHAIN_NAME&gt;
</code></pre>
<h4 id="glossary">Glossary</h4>
<h5 id="non-eks-ec2-compute-amis">Non-EKS EC2 Compute AMIs</h5>
<ul>
<li>Check that the following system network configurations are the indicated values. If not, update the sys config to these values on non-EKS provided AMI</li>
</ul>
<pre><code class="language-sh">`sudo sysctl net.ipv4.ip_forward` : 1
`sudo sysctl net.bridge.bridge-nf-call-iptables` : 1
`sudo systctl net.ipv4.conf.all.rp_filter` : 1
</code></pre>
<ul>
<li><a href="https://commons.wikimedia.org/wiki/File:Netfilter-packet-flow.svg">Linux Networking reference</a></li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href=".." class="md-footer__link md-footer__link--prev" aria-label="Previous: Home">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Home
              </div>
            </div>
          </a>
        
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.footer"], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.88dd0f4e.min.js"></script>
      
    
  </body>
</html>