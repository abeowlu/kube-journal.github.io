{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to this random tech blog","text":"<p>If you've stumbled on this yet most random of pages, instead of being delivered carefully by some algorithm directly to the page you intend to visit, well... welcome.</p> <p>These articles discuss varying kubernetes components and implementations.</p> <ul> <li>Manual Setup of Kubernetes Services using IPtables</li> <li>Manual Setup of Kubernetes Services using IPVS</li> <li>[Commone Trobuleshooting Patterns]</li> </ul>"},{"location":"article8-netwrking/","title":"Manual Setup of Kubernetes Services using IPtables","text":"<p>TThis is a series of discussions that builds off Dola's article on Kubernetes networking and iptables. We take a look at a hands-on implementation of a custom cluster networking, to fully understand how Kubernetes sets up its own;</p> <ul> <li>iptables configuration for pod-to-pod, and pod-to-external-endpoint connections, </li> <li>ipvs and nftable implementation, replacing iptables - to reduce the number of iptables rules required, which could grow very large, </li> <li>and round off with troubleshooting suggestion for aws-node(CNI) liveness/readiness probe errors, and component failures connecting to <code>kubernetes.svc</code> API endpoint within cluster.</li> </ul>"},{"location":"article8-netwrking/#implementation","title":"Implementation","text":"<p>This hands-on tutorial starts by building 2 pod network sandboxes manually, and pointing out what part of this process would be implemented by Kubelet, CRI, CNI and kube-proxy. It is recommended to try this on an cluster worker-node with some pods running, to be able to contrast the manual pods with some Kubernetes pod under k8s.io contaunerd namespace, and also to take advantage of some system network configuration, but not a requirement.</p> <p>Some simplification we introduce in creating a pod eliminates certain Kubernetes complexities during pod creation, like;</p> <ul> <li>kubelet creating a podA.slice CPU cgroup under /run/systemd/system/kubepods.slice hierarchy, instead we use the default user slice for the manual pods</li> <li>Container runtime interface (CRI) creating a sandbox for pod under /var/lib/containerd/io.containerd.runtime.v2.task/default/, instead we use the \"default\" containerd pod namespace</li> <li>we however, do the container network interface (CNI) tasks of creating a network namespace (net-ns), managing IP, and wiring up the network, similarly to how they are done on Kubernetes clusters. This is a very CNI-dependent process, but the networking expectations remain the same.</li> <li>then we run a container task in these network namespaces. So let's dive right in shall we.</li> </ul>"},{"location":"article8-netwrking/#configure-the-networking","title":"Configure The Networking","text":"<p>Let's start with a few handy commands to take a look at the current network namespaces(net-ns) on the computer/worker-node, and the network interfaces. We will run these commands often as we progress to see the changes configured. It is possible to already have net namespaces(net-ns) built by a cluster CNI, e.g, cni-ABCDEFG on a worker node.</p> <pre><code>sudo ip netns\nsudo ip link show\n</code></pre> <ol> <li>We create two new network namespaces(net-ns) of our own for the pods, <code>pod-1</code> and <code>pod-2</code>, that we will create manually. We will call these namespaces <code>veth-ns-test-pod-1</code> and <code>veth-ns-test-pod-2</code>. Nomenclature is to indicate we are using kernel virtual ethernet devices. Run the handy check commands above after this configuration to see the changes in current net-ns again.</li> </ol> <pre><code>sudo ip netns add veth-ns-test-pod-1\nsudo ip netns add veth-ns-test-pod-2\n</code></pre> <ol> <li>Next, we create the Linux virtual ethernet network devices used for by the pod sandbox, and at the same time, the peer for these veth interfaces.</li> </ol> <p>|    a. run the handy checks to view the current devices on the compute after the configuration below</p> <pre><code>sudo ip link add veth-pod-1 type veth peer name veth-peer-pod-1\nsudo ip link add veth-pod-2 type veth peer name veth-peer-pod-2\n</code></pre> <ol> <li>In no particular order, we need to move the <code>veth-pod-1</code> and <code>veth-pod-2</code> devices into the network namespaces, <code>veth-ns-test-pod-1</code> and <code>veth-ns-test-pod-2</code>, respectively. This could have been done with step 2 above, but for clarity, we do this separately.</li> </ol> <p>|    a. again run the handy check after the configuration below, and <code>veth-pod-1@veth-peer-pod-1</code> and <code>veth-pod-2@veth-peer-pod-2</code> interfaces should no longer be listed in the host's network, while the peers are. The veth interfaces have been moved into distinct net-ns.</p> <pre><code>sudo ip link set dev veth-pod-1 netns veth-ns-test-pod-1\nsudo ip link set dev veth-pod-2 netns veth-ns-test-pod-2\n</code></pre> <p>|    b. To see the veth devices in their respective namespaces, run;</p> <pre><code>sudo ip netns exec veth-ns-test-pod-1 ip link show\nsudo ip netns exec veth-ns-test-pod-2 ip link show\n</code></pre> <ol> <li>Notice in the command above that the Loopback interface in the network namespaces are currently DOWN. We need to turn this <code>UP</code> or basic 127.0.0.1 network connections; e.g self-health check on localhost:8080/healthz would fail for the processes(pods) in these network namespaces</li> </ol> <pre><code>sudo ip netns exec veth-ns-test-pod-1 ip link set dev lo up\nsudo ip netns exec veth-ns-test-pod-2 ip link set dev lo up\n</code></pre> <ol> <li>Also, notice that the veth interfaces in the custom namespaces, and the peers in the host network namespace are also DOWN, lets set all these up;</li> </ol> <pre><code>sudo ip link set dev veth-peer-pod-1 up\nsudo ip link set dev veth-peer-pod-2 up\n\nsudo ip netns exec veth-ns-test-pod-1 ip link set dev veth-pod-1 up\nsudo ip netns exec veth-ns-test-pod-2 ip link set dev veth-pod-2 up\n</code></pre>"},{"location":"article8-netwrking/#create-manual-pods-1-2","title":"Create Manual Pods 1 &amp; 2","text":"<p>At this point, we can a rough pod networking setup and can rollout test containers into pod sandboxes using these net-ns and poke around inside the pod. This should reveal the current networking limitations, as the work of the CNI is mostly done, but we haven't wired up the network yet, no IP assigned to pod, no routes for pod-to-pod or external endpoints, etc., we can then fix these limitations.</p> <ol> <li>We create two container/task sandboxes (pods) in a similar directory where containerd(CRI) creates Kubernetes pods sandboxes. You should look around this directory if desired to see running pods, if you have them.</li> </ol> <pre><code>sudo mkdir -p /run/containerd/pod-1/\nsudo mkdir -p /run/containerd/pod-2/\n</code></pre> <p>|   a. we are going to pull a trusted nginx container image as the base image for all our containers, and create a container spec -  a container config.json specifies how runc initialized a container. This is similar to kubelet invoking the CRI, which pulls and validates the pod container images.</p> <pre><code>sudo ctr image pull docker.io/nginxinc/nginx-unprivileged:latest &amp;&amp; \\\nsudo ctr run --detach docker.io/nginxinc/nginx-unprivileged:latest default-nginx\n\nsudo cp -r /run/containerd/io.containerd.runtime.v2.task/default/default-nginx/rootfs /run/containerd/pod-1/\nsudo cp -r /run/containerd/io.containerd.runtime.v2.task/default/default-nginx/rootfs /run/containerd/pod-2/\n\ncd /run/containerd/pod-2/ &amp;&amp; sudo runc spec \ncd /run/containerd/pod-1/ &amp;&amp; sudo runc spec\n</code></pre> <p>NB: you can always check on what config has changed in the container spec at any point, by making a backup now for later comparisons</p> <pre><code>sudo cp config.json config.json.bkp\nsudo diff -y config.json.bkp config.json\n</code></pre> <p>|   b. update the container spec with the manually created network namespaces for both pod-1 and pod-2, and and start the container using runc. This is exactly how containerd starts containers for kubelet as well. Created network namespaces are located in <code>/var/run/netns</code></p> <pre><code>echo $(jq '(.linux.namespaces[] | select(.type == \"network\")) += \\\n{\"path\": \"/var/run/netns/veth-ns-test-pod-1\"}' /run/containerd/pod-1/config.json)  \\\n| sudo tee -i /run/containerd/pod-1/config.json\n\necho $(jq '(.linux.namespaces[] | select(.type == \"network\")) += \\\n{\"path\": \"/var/run/netns/veth-ns-test-pod-2\"}' /run/containerd/pod-2/config.json) \\\n| sudo tee -i /run/containerd/pod-2/config.json\n</code></pre> <p>|   c. let us also set the default nginx startup instructions to ensure the servers start, this is the pod manifest <code>.spec.containers[].command</code> in kubernetes. Also, allow container root access (not strictly required for the nginx image used), and add some Linux CAPs, these are necessary for nginx servers to start up with required network capabilities;</p> <pre><code>sudo echo $(jq '.process.capabilities.bounding += [ \"CAP_CHOWN\", \"CAP_SETGID\", \"CAP_SETUID\" ] \\\n| .process.capabilities.permitted += [ \"CAP_CHOWN\", \"CAP_SETGID\", \"CAP_SETUID\" ] \\\n| .root.readonly = false | .process.args = [ \"/docker-entrypoint.sh\", \"nginx\", \"-g\", \"daemon off;\" ]' /run/containerd/pod-1/config.json)  \\\n| sudo jq '.' | sudo tee -i /run/containerd/pod-1/config.json\n\nsudo echo $(jq '.process.capabilities.bounding += [ \"CAP_CHOWN\", \"CAP_SETGID\", \"CAP_SETUID\" ] \\\n| .process.capabilities.permitted += [ \"CAP_CHOWN\", \"CAP_SETGID\", \"CAP_SETUID\" ] \\\n| .root.readonly = false | .process.args = [ \"/docker-entrypoint.sh\", \"nginx\", \"-g\", \"daemon off;\" ]' /run/containerd/pod-2/config.json) \\\n| sudo jq '.' | sudo tee -i /run/containerd/pod-2/config.json\n</code></pre> <p>|   d. Next, we now run the containers. The way we invoke runc without providing file descriptors, we need to start different shell sessions for each container.</p> <pre><code>cd /run/containerd/pod-1/ &amp;&amp; sudo runc run pod-1\ncd /run/containerd/pod-2/ &amp;&amp; sudo runc run pod-1\nsudo runc list\n</code></pre> <p>Runc Terminal Handling</p> <p>In order to keep this discussion streamlined, we are going to let runc reparent the container process to the calling shells  - in other words, our current terminal will be captured as stdout and stderr for the containers. The popular kubernetes CRI, containerd, CRI-O, etc. manages container runtime by implementing a wrapper around runc and providing a socket file descriptor that runc sets the container up with - this is what powers functionality such as <code>kubectl logs</code> and <code>kubectl exec -it</code></p> <p>|   a. To poke arround the pod environment - check the /proc/net directory of the container task, and do the same for pod-2- retrieve the pid of the container that is started - the PID can be used to enter the process' namespaces, with <code>nsenter -n -t &lt;PID&gt;</code> command, as runc has exited and parented the current shell to the container, there is no other way to exec back into this running (*pod)container. For the simplicity of our conversation we will use the <code>ip netns</code> to exec into the process namespace without needing to look up PIDs going forward</p> <pre><code>sudo runc list\nsudo nsenter -n -t &lt;PID&gt; curl localhost:8080\nsudo nsenter -n -t &lt;PID&gt; curl amazon.com\nsudo nsenter -n -t &lt;PID&gt; cat /proc/net/route\n</code></pre> <p>The <code>localhost</code> connection succeeds from the command above, while the connection to an external <code>amazon.com</code> endpoint fails with <code>Could not resolve host amazon.com</code> - as there are no routes set up. So, lets us finish wiring up the pod networking, as a CNI would.</p>"},{"location":"article8-netwrking/#wiring-up-the-pod-network","title":"Wiring Up The Pod Network","text":"<ol> <li>we continue the setup by assigning IPs to both pod-1 and pod-2, from arbitrary private CIDR, 100.64.0.0/16, with minimal chance of collisions with any network. This is the functionality of an IPAM in cluster network. Note that we are using a virtual pod IP range, other CNIs, for example, aws-node CNI, uses real IP from the VPC subnets for a more resource-expensive but faster and better external networking.</li> </ol> <pre><code>sudo ip netns exec veth-ns-test-pod-1 ip addr add 100.64.0.9/24 dev veth-pod-1\nsudo ip netns exec veth-ns-test-pod-2 ip addr add 100.64.0.10/24 dev veth-pod-2\n</code></pre> <ol> <li>we create the pod-to-pod connectivity next using a bridge device. Docker uses a similar bridge device for container connections, different CNIs implement this differently;</li> </ol> <ul> <li>aws CNI uses a dummy interface for pod-to-pod routing, real VPC IPs are used, so in-VPC and external networking routes setup is not required</li> <li>calico/weave uses a tunnel device for pod-to-pod connection, virtual IPs are used, which can be IP resource cheap but expensive routing and connecting to external endpoints</li> </ul> <p>|   a. bring up a bridge device for the in-cluster connections</p> <pre><code>sudo ip link add custom-cni type bridge \n</code></pre> <p>|   b. most CNIs use the default gateway address of 169.254.1.1 in pod namespaces. However, for clarity we use the 100.64.0.1 of the pod virtual IP CIDR range</p> <pre><code>sudo ip addr add 100.64.0.1/24 dev custom-cni\nsudo ip link set custom-cni up\n</code></pre> <p>|   c. plug the veth peer devices of the pods currently in the host network namespace into the custom-cni bridge device - check the routes in pod net-ns (<code>cat /proc/net/route</code> or <code>ip route</code>) and notice the net manager has configured the routes wired up, also notice veth-peer in host net-ns has been set to be promiscuous (<code>ip -details link show veth-peer-pod-1</code>)</p> <pre><code>sudo ip link set dev veth-peer-pod-1 master custom-cni\nsudo ip link set dev veth-peer-pod-2 master custom-cni\n</code></pre> <p>|    d. lets add a default route to send traffic through the bridge, </p> <pre><code>sudo ip netns exec veth-ns-test-pod-1 ip route add default via 100.64.0.1\nsudo ip netns exec veth-ns-test-pod-2 ip route add default via 100.64.0.1\n</code></pre> <p>|   e. if both containers not already started in step 7e., run pod-1 and pod-2 in 2 separate terminals - <code>cd /run/containerd/pod-2/ &amp;&amp; sudo runc run pod-2</code>. Open a 3rd terminal and we can test connectivity from one pod to the second pod, and itself</p> <pre><code>sudo ip netns exec veth-ns-test-pod-1 curl -ikv 100.64.0.9:8080\nsudo ip netns exec veth-ns-test-pod-1 curl -ikv 100.64.0.10:8080\nsudo ip netns exec veth-ns-test-pod-1 curl -ikv amazon.com\n</code></pre> <p><code>Troubleshooting tip</code>: You can always diagnose pod network status and issues by running a tcpdump on interfaces in the pod network namespaces, while making inbound or outbound connections from yet another terminal, then analyze packet capture with;</p> <pre><code>sudo ip netns exec veth-ns-test-pod-1 tcpdump -vvv -i any -w pod1.pcap\nsudo tcpdump -r pod1.pcap\n</code></pre> <p>The pod network connection to itself and connection from pod-1 to pod-2 and vice-versa, using assigned IPs succeed, but external connection to <code>amazon.com</code> continues to fail. Depending on familiarity with iptables, Dola's article may be appropriate requisite reading, but we will highlight iptables usage here enough for our discussion as well.</p> <p>Why does pod-to-pod connections succeed after setting up the bridge? Well, in a pod-to-pod connection on the same host, packets are flowing from one Linux process to another, and not using any external network interfaces. The bridge we created loads with netfilter_call mode enabled by default, and from the perspective of the iptable traversal, packet flow would look like;</p> <p></p> <p>NB: Troubleshooting Issues</p> <p>if you have unexpected connection issues at this point. You may not be using a cluster workernode, which would have some necessary network configurations pre-implemented. See glossary section on loading bridge with net.bridge.bridge-nf-call-iptables on non-cluster computers. And check the troubleshooting tips in the article to get grease under your finger nails. </p> <ul> <li> <p>the raw iptables chains aren't often used by most processes for good reason, and are usually pass through by default. An example reason, a process' packet that should be monitored by conntrack should not match rules in the raw-OUTPUT. This table is invoked before conntrack netfilter hooks are called by the kernel. While the other chains allow the packet to flow;</p> </li> <li> <p><code>sudo iptables -L FORWARD -t mangle</code>- mangle-FORWARD is ACCEPT by default and open</p> </li> <li><code>sudo iptables -L FORWARD -t filter</code> - filter-FORWARD is ACCEPT by default for packet not matching rules in the chain. </li> </ul> <p>We can make a quick test to block packets from pod-1(100.64.0.9/32) specifically, while pod-2 is still allowed</p> <pre><code>sudo iptables -A FORWARD -t filter -s 100.64.0.9/32 -j DROP\nsudo ip netns exec veth-ns-test-pod-1  curl -ikv 100.64.0.10:8080\n</code></pre> <ul> <li>allow connection from pod-1 again by removing this firewall rule;</li> </ul> <pre><code>sudo iptables -D FORWARD -s 100.64.0.9/32 -j DROP\n</code></pre> <p>NB: nodeLocalDNS is an example of a kubernetes component that does use raw-OUTPUT, which would cause its traffic not to appear in the conntrack table.</p> <p>Next, we complete setting up our custom network paths, as pictured below, on the compute node. At any point, you can perform the actions in <code>Troubleshooting tip</code> above and analyze captured packets to see its flow;</p> <p></p> <p>The last CNI task we perform is set up pod-to-external destination network routes. Then we perform the kube-proxy component's function by creating custom virtual services to expose our custom pods, pod-1 and pod-2 containers, simulating Kubernetes clusterIP service, to round off this discussion.</p> <ol> <li>we set up iptables rule to allow communication from our cluster custom-cni bridge interface on a network range of 100.64.0.0/16 to connect externally through the current VPC, and whatever network range it has. From the perspective of iptable traversal, packet flow would look like;</li> </ol> <p></p> <ul> <li>the switch from our custom-cni bridge interface to the node's primary interface in ROUTING DECISIONS is performed by the kernel netwrok module</li> <li><code>sudo iptables -L OUTPUT -t mangle &amp;&amp; sudo iptables -L POSTROUTING -t mangle</code> are open chains allowing free packet flow by default, as are the others</li> <li>the packets from the pods are egressing into the VPC using the private virtual IPs we assigned - 100.64.0.9 and 100.64.0.10. Amazon VPC, and most network will reject/drop this packet with unknown source IP address by default for (AWS) security reasons. We need to SNAT pod traffic to external destinations similar to aws CNI AWS-SNAT-CHAIN rules. There are a number of ways to achieve this SNAT, with simplicity as our main consideration - we will implement it in the last chain above, nat-POSTROUTING;</li> </ul> <p>|   a. retrieve the primary private IP of your worker node</p> <pre><code>nodeIP=$(sudo ip -details -j addr show | jq '.[].addr_info[] | select(.label == \"eth0\") | .local') &amp;&amp; localIP=$(sudo echo ${nodeIP} | tr -d '\"')\n</code></pre> <p>|   b. we insert a rule in the nat-POSTROUTING chain, matching packet from the pod CIDR, and tracking connections to external destination, i.e. not local, and SNAT the packet - change source IP address from the pod IP to the host IP. </p> <pre><code>sudo iptables -I POSTROUTING -t nat -s 100.64.0.0/16 -m addrtype ! --dst-type LOCAL -m conntrack --ctstate NEW,ESTABLISHED -j SNAT --to-source ${localIP}\n</code></pre> <p>Test: <code>sudo ip netns exec veth-ns-test-pod-1 curl -ikv amazon.com</code></p> <ol> <li>Lastly, we use iptables to expose the pod-1, 100.64.0.9 and pod-2, 100.64.0.10, behind virtual service IPs which we will pick from an arbitraru range of 198.19.0.0/16, with minimal collision chance. We will stick close to how kube-proxy sets up a similar clusterIP service type. First, the iptable traversal from pod-1 to service-2 delivered to pod-2 looks like this;</li> </ol> <p></p> <p>|   a. create a custom network service chains, CUSTOM-CLUSTER-SERVICES, CUSTOM-SERVICES-1 and CUSTOM-SERVICES-2 in the nat table, synonymous to the KUBE-SERVICES chain created by kube-proxy which is used for clear rules management. Then, create the required service endpoint (SEP) chain for service-to-pod routing, also in the nat table;</p> <pre><code>sudo iptables -t nat -N CUSTOM-CLUSTER-SERVICES\nsudo iptables -t nat -N CUSTOM-SERVICES-1\nsudo iptables -t nat -N CUSTOM-SERVICES-2\nsudo iptables -t nat -N CUSTOM-SEP-1\nsudo iptables -t nat -N CUSTOM-SEP-2\n</code></pre> <p>|   b. importantly, we inform the main iptables nat-PREROUTING and nat-OUTPUT chains to look through our nat-CUSTOM-CLUSTER-SERVICES chain during packet processing. If you don't add the nat-OUTPUT rule here, connection from the host to service IP 198.19.0.9:8080 fails, but pod-x-to-service-y succeeds, try to understand what happened to the node's packets to the pod, use the troubleshooting tip and iptables flow diagram above.</p> <pre><code>sudo iptables -I PREROUTING -t nat -m comment --comment \"custom-cluster-services\" -j CUSTOM-CLUSTER-SERVICES\nsudo iptables -I OUTPUT -t nat -m comment --comment \"custom-cluster-services\" -j CUSTOM-CLUSTER-SERVICES\n</code></pre> <p>|   c. next, we configure CUSTOM-CLUSTER-SERVICES to look through CUSTOM-SERVICES-1 and CUSTOM-SERVICES-2 chains, we can restrict the network protocols, e.g., tcp(6), udp(17), etc., that the service endpoint accepts in this chain, like kube-proxy, or in any of the later chains. But we leave it open to all protocols for simplicity.</p> <pre><code>sudo iptables -A CUSTOM-CLUSTER-SERVICES -t nat -d 198.19.0.9 -m comment --comment \"Rule to service-1 -&gt; 198.19.0.9\" -j CUSTOM-SERVICES-1\nsudo iptables -A CUSTOM-CLUSTER-SERVICES -t nat -d 198.19.0.10 -m comment --comment \"Rule to service-2 -&gt; 198.19.0.10\" -j CUSTOM-SERVICES-2\n</code></pre> <p>|   d. configure the iptables rule in CUSTOM-SERVICES-1 and CUSTOM-SERVICES-2 to look through the service endpoint (SEP) chains, CUSTOM-SEP-1 and CUSTOM-SEP-2, this is another rule that could be eliminated from the stack, but kube-proxy uses it for readability and rule management, and we are sticking with kube-proxy implementation, so;</p> <pre><code>sudo iptables -A CUSTOM-SERVICES-1 -t nat -m comment --comment \"Rule to service-1-endpoionts\" -j CUSTOM-SEP-1  \nsudo iptables -A CUSTOM-SERVICES-2 -t nat -m comment --comment \"Rule to service-2-endpoionts\" -j CUSTOM-SEP-2 \n</code></pre> <p>|   e. for the virtual service IP functionality - sending all traffic received at service IP to endpoint pod IP - configure the following rules in the CUSTOM-SEP-X chains for <code>198.19.0.9 to 100.64.0.9</code> and <code>198.19.0.10 to 100.64.0.10</code></p> <pre><code>sudo iptables -A CUSTOM-SEP-1 -t nat -p tcp -j DNAT --to-destination 100.64.0.9:8080\nsudo iptables -A CUSTOM-SEP-2 -t nat -p tcp -j DNAT --to-destination 100.64.0.10:8080\n</code></pre> <p>|   f. test connectivity in all directions, from the node to the service IPs, from pod-1 to service-2 IP, from pod-1 to service-1 IP</p> <pre><code>curl -ikv 198.19.0.9:8080\ncurl -ikv 198.19.0.10:8080\nsudo ip netns exec veth-ns-test-pod-1 curl 198.19.0.10:8080\n...\n</code></pre> <p>One last, thing for the sake of completeness. Note that pod-1 and pod-2 are unable to connect to itself using the service IP exposing them. Test connection from a pod-1 through service-1 IP exposing it, and for pod-2;</p> <pre><code>sudo ip netns exec veth-ns-test-pod-2  curl -ikv 198.19.0.10:8080\n</code></pre> <p><code>Troubleshooting tip</code>: You can check the troubleshooting article in this series of discussion for in-depth look. To understand what is occurring to our packets, here, however, let us check the flow, open 3 new terminals</p> <pre><code>terminal 1: `sudo tcpdump -vvv -n host 198.19.0.10 -i custom-cni`\nterminal 2: `sudo ip netns exec veth-ns-test-pod-2 tcpdump -vvv -n host 198.19.0.10 -i any`\nterminal 3: `sudo ip netns exec veth-ns-test-pod-2 curl -ikv 198.19.0.10:8080`\n</code></pre> <p>In terminal 1, note that pod-2 is sending SYN packets to the <code>198.19.0.10.webcache</code> endpoint, that are never ACK'd. In terminal 2, the custom-cni bridge is discarding the packet that should be sent from <code>198.19.0.10</code> back to the originating pod-2. This issue is occurring because the bridge does not allow hairpin traffic by default. To correct this, check the current bridge interface configuration and enable promiscuous mode with;</p> <pre><code>sudo ip -details link show custom-cni\nsudo ip link set custom-cni promisc on\n</code></pre> <p>At this point, we have manually created 2 pods running nginx containers and set up a full pod networking on a compute/worker node. Implementing the specification of the OCI CNI requirements](https://github.com/containernetworking/cni) manually, to better understand how cluster networks. To top it off, we have exposed these pods behind our own service IPs within the cluster. In the next discussion, we expose these pods publicly, and look into the benefits of using the newer IPVS mode of kube-proxy over iptables rules management.</p> <p>Finally, if desired, you can clean up all the implementation above;</p> <pre><code>##Stopping the containers\nsudo runc kill pod-1 KILL \n\n## Deleting the chains\nsudo iptables -t nat -D &lt;$CHAIN_NAME&gt;\n</code></pre>"},{"location":"article8-netwrking/#glossary","title":"Glossary","text":""},{"location":"article8-netwrking/#non-eks-ec2-compute-amis","title":"Non-EKS EC2 Compute AMIs","text":"<ul> <li>Check that the following system network configurations are the indicated values. If not, update the sys config to these values on non-EKS provided AMI</li> </ul> <pre><code>`sudo sysctl net.ipv4.ip_forward` : 1\n`sudo sysctl net.bridge.bridge-nf-call-iptables` : 1\n`sudo systctl net.ipv4.conf.all.rp_filter` : 1\n</code></pre> <ul> <li>Linux Networking reference</li> </ul>"},{"location":"article9-netwrk2/","title":"Manual Setup of Kubernetes Services using IPVS","text":"<p>In the first discussion in this series, we created a custom cni networking for 2 pod containers, and exposed them behind 2 virtual services (clusterIP). That discussion is requisite to this one - where we look into the benefits of IPVS mode over iptables mode in cluster orchestration, and replace our iptables rules implementation with IPVS rule sets in the custom container network.</p>"},{"location":"article9-netwrk2/#implementation","title":"Implementation","text":"<p>We will start by adding 2 more pods to our custom network, with each pod added as a second endpoint behind the existing custom services, service-1 and service-2. Then we expose these services externally on a nodePort, to understand how kube-proxy sets this up for kubernetes clusters. And optionally expose the nodePorts behind an elastic loadbalancer for public connections. This should hopefully give us deep insight into our iptables rule management grows for each additional service and pod(s). Then we replace our iptables configuration with IPVS to use the benefits of the latter that we discuss, for kernel packet distribution.</p>"},{"location":"article9-netwrk2/#create-pod-3-4-sandboxes","title":"Create Pod 3 &amp; 4 Sandboxes","text":"<ol> <li>we add 2 more pod sandboxes, pod-3 and pod-4 to our cluster, by running a few of the steps in the first discussion.</li> </ol> <p>|   a. networking configuration commands;</p> <pre><code>sudo ip netns add veth-ns-test-pod-3\nsudo ip netns add veth-ns-test-pod-4\n\nsudo ip link add veth-pod-3 type veth peer name veth-peer-pod-3\nsudo ip link add veth-pod-4 type veth peer name veth-peer-pod-4\n\nsudo ip link set dev veth-pod-3 netns veth-ns-test-pod-3\nsudo ip link set dev veth-pod-4 netns veth-ns-test-pod-4\n\nsudo ip netns exec veth-ns-test-pod-3 ip addr add 100.64.0.19/24 dev veth-pod-3\nsudo ip netns exec veth-ns-test-pod-4 ip addr add 100.64.0.20/24 dev veth-pod-4\n\nsudo ip link set dev veth-peer-pod-3 master custom-cni\nsudo ip link set dev veth-peer-pod-4 master custom-cni\n\nsudo ip netns exec veth-ns-test-pod-3 ip link set dev lo up\nsudo ip netns exec veth-ns-test-pod-4 ip link set dev lo up\n\nsudo ip link set dev veth-peer-pod-3 up\nsudo ip link set dev veth-peer-pod-4 up\n\nsudo ip netns exec veth-ns-test-pod-3 ip link set dev veth-pod-3 up\nsudo ip netns exec veth-ns-test-pod-4 ip link set dev veth-pod-4 up\n\nsudo ip netns exec veth-ns-test-pod-3 ip route add default via 100.64.0.1\nsudo ip netns exec veth-ns-test-pod-4 ip route add default via 100.64.0.1\n</code></pre> <p>|    b. create pod-3 and pod-4 containers, and we misuse our root privileges on a cluster worker node to edit container filesystem, displaying a page that indicates the pod IP behind the fronting service, 1 or 2, that receives our connections traffic when we access the nginx server. If it is not immediately apparent why this is a security issue, then it is likely for the best. However, this shows why it is a security concern that pod applications in multi-tenant, no-trust clusters, or any cluster at all, never break out unto the node.</p> <pre><code>sudo mkdir -p /run/containerd/pod-3/\nsudo mkdir -p /run/containerd/pod-4/\n\npod1=$(sudo sudo ip netns exec veth-ns-test-pod-1 ip -details -j addr show | jq '.[].addr_info[] | select(.label == \"veth-pod-1\") | .local')\npod3=$(sudo sudo ip netns exec veth-ns-test-pod-3 ip -details -j addr show | jq '.[].addr_info[] | select(.label == \"veth-pod-3\") | .local')\nsudo sed -i \"s/&lt;p&gt;If/&lt;p&gt;Pod-1 Endpoint From Service-1 Pod-IP: $pod1 \\n\\n\\nIf/1\" /run/containerd/pod-1/rootfs/usr/share/nginx/html/index.html\nsudo cp -r /run/containerd/pod-1/rootfs /run/containerd/pod-3\nsudo sed -i \"s/Pod-1/Pod-3/;s/$pod1/$pod3/g\" /run/containerd/pod-3/rootfs/usr/share/nginx/html/index.html\n\npod2=$(sudo sudo ip netns exec veth-ns-test-pod-2 ip -details -j addr show | jq '.[].addr_info[] | select(.label == \"veth-pod-2\") | .local')\npod4=$(sudo sudo ip netns exec veth-ns-test-pod-4 ip -details -j addr show | jq '.[].addr_info[] | select(.label == \"veth-pod-4\") | .local')\nsudo sed -i \"s/&lt;p&gt;If/&lt;p&gt;Pod-2 Endpoint From Service-2 Pod-IP: $pod2 \\n\\n\\nIf/1\" /run/containerd/pod-2/rootfs/usr/share/nginx/html/index.html\nsudo cp -r /run/containerd/pod-2/rootfs /run/containerd/pod-4\nsudo sed -i \"s/Pod-2/Pod-4/;s/$pod2/$pod4/g\" /run/containerd/pod-4/rootfs/usr/share/nginx/html/index.html\n\ncd /run/containerd/pod-3/ &amp;&amp; sudo runc spec \ncd /run/containerd/pod-4/ &amp;&amp; sudo runc spec\n\necho $(jq '(.linux.namespaces[] | select(.type == \"network\")) += {\"path\": \"/var/run/netns/veth-ns-test-pod-3\"}' /run/containerd/pod-3/config.json)  | sudo tee -i /run/containerd/pod-3/config.json\necho $(jq '(.linux.namespaces[] | select(.type == \"network\")) += {\"path\": \"/var/run/netns/veth-ns-test-pod-4\"}' /run/containerd/pod-4/config.json)  | sudo tee -i /run/containerd/pod-4/config.json\n\nsudo echo $(jq '.process.capabilities.bounding += [ \"CAP_CHOWN\", \"CAP_SETGID\", \"CAP_SETUID\" ] \\\n| .process.capabilities.permitted += [ \"CAP_CHOWN\", \"CAP_SETGID\", \"CAP_SETUID\" ] \\\n| .root.readonly = false | .process.args = [ \"/docker-entrypoint.sh\", \"nginx\", \"-g\", \"daemon off;\" ]' /run/containerd/pod-3/config.json)  \\\n| sudo jq '.' | sudo tee -i /run/containerd/pod-3/config.json\n\nsudo echo $(jq '.process.capabilities.bounding += [ \"CAP_CHOWN\", \"CAP_SETGID\", \"CAP_SETUID\" ] \\\n| .process.capabilities.permitted += [ \"CAP_CHOWN\", \"CAP_SETGID\", \"CAP_SETUID\" ] \\\n| .root.readonly = false | .process.args = [ \"/docker-entrypoint.sh\", \"nginx\", \"-g\", \"daemon off;\" ]' /run/containerd/pod-4/config.json)  \\\n| sudo jq '.' | sudo tee -i /run/containerd/pod-4/config.json\n\ncd /run/containerd/pod-3 &amp;&amp; sudo runc run pod-3\n---\n#Open a new terminal and start pod-4\ncd /run/containerd/pod-4 &amp;&amp; sudo runc run pod-4\n</code></pre> <ol> <li>with the pods' containers running and reachable within cluster network, we are going to add pod-3 as a second endpoint behind service-1 and pod-4 as behind service-2. Kube-proxy adds endpoint by watching for new endpoint/slice on the API server. Then builds the appropriate proxy rules, and updates Iptables chains on every node.</li> </ol> <p>|   a. in our manual implementation, we simply could add our new pod IPs as endpoints in CUSTOM-SEP-1 and CUSTOM-SEP-2 chains. However, for readability and adhering to how kube-proxy implements routing; we will create 2 new service endpoints SEP-3 and SEP-4, dedicated to the new pod-3 and pod-4</p> <pre><code>sudo iptables -t nat -N CUSTOM-SEP-3\nsudo iptables -t nat -N CUSTOM-SEP-4\n\nsudo iptables -A CUSTOM-SEP-3 -t nat -p tcp -j DNAT --to-destination 100.64.0.19:8080\nsudo iptables -A CUSTOM-SEP-4 -t nat -p tcp -j DNAT --to-destination 100.64.0.20:8080\n</code></pre> <p>|   b. we then insert the probabilistic routing rules, pairing the odd and even number pods, such that CUSTOM-SERVICES-1 now also routes to CUSTOM-SEP-3, and CUSTOM-SERVICES-2 chains routes to CUSTOM-SEP-4.</p> <pre><code>sudo iptables -t nat -I CUSTOM-SERVICES-1 -m comment --comment \"Rule to service-1-endpoionts 50% Probability\" -m statistic --mode random --probability 0.5 -j CUSTOM-SEP-3\nsudo iptables -t nat -I CUSTOM-SERVICES-2 -m comment --comment \"Rule to service-2-endpoionts 50% Probability\" -m statistic --mode random --probability 0.5 -j CUSTOM-SEP-4\n</code></pre> <p>|   c. we test the random distribution of traffic by connecting to service-1 or 2, ten times and checking how many are routed to either backend pods behind the service we connect to. Iptables load balancing works using a random probabilistic distribution to choose which endpoint is sent a connection.</p> <pre><code>for i in {1..10}; do curl -ikvs 198.19.0.10:8080/ 2&gt;/dev/null; done | grep \"Pod-4\" | wc -l\nfor i in {1..10}; do curl -ikvs 198.19.0.9:8080/ 2&gt;/dev/null; done | grep \"Pod-3\" | wc -l\n</code></pre> <p>Traffic from external sources, node, ELB, etc. to a pod on a node would traverse the iptables chains below; </p>"},{"location":"article9-netwrk2/#expose-node-port-optional-configure-elb-access","title":"Expose Node Port (Optional: Configure ELB Access)","text":"<p>Connecting to service-1 on <code>198.19.0.9:8080</code>, in the test above, for example, the endpoints that get the traffic is completely random, and you will get different response distribution from pod-1 and pod-3 each time. To perhaps make this a bit more visual, and build on our understanding of kube-proxy cluster networking, we will create an iptables rule simulating a kubernetes nodePort service type and optionally expose it behind an Elastic Loadbalancer that we can connect to from a web browser and see what pods are responding.</p> <ol> <li>we are going to choose arbitrary static node ports with minimal chance of conflicts with any other process on the computer, as well as Kubernetes' kube-proxy default port range: 30000-32767. For this configuration, nodePort-1 exposing pod-1 and pod-3 will be on port 65509, and nodePort-2 exposing pod-2 and pod-4 will be on port 65510.</li> </ol> <p>|   a. again, we can create a single chain pointing to the CUSTOM-SERVICES-1 or 2 chains, but to implement this the way kube-proxy does, we are going to create a NODEPORT chain pointing to an EXT chain for external connection, which then points to the CUSTOM-SERVICE chains, which points to the SEP service endpoint destinations. For the traffic we intend to expose externally, we enforce tcp (6) traffic protocol in these flows for security.</p> <pre><code>sudo iptables -t nat -N CUSTOM-NODEPORTS\nsudo iptables -t nat -N CUSTOM-EXT-SERVICE-1\nsudo iptables -t nat -N CUSTOM-EXT-SERVICE-2\n\nsudo iptables -t nat -A CUSTOM-CLUSTER-SERVICES -m comment --comment \"NodePort rules appended in CLUSTER-SERVICES chain\" -j CUSTOM-NODEPORTS\nsudo iptables -t nat -A CUSTOM-NODEPORTS -m comment --comment \"Nodepoort to service-1 pods\" -p tcp --dport 65509 -j CUSTOM-EXT-SERVICE-1\nsudo iptables -t nat -A CUSTOM-NODEPORTS -m comment --comment \"Nodepoort to service-2 pods\" -p tcp --dport 65510 -j CUSTOM-EXT-SERVICE-2\n\nsudo iptables -t nat -A CUSTOM-EXT-SERVICE-1 -j CUSTOM-SERVICES-1\nsudo iptables -t nat -A CUSTOM-EXT-SERVICE-2 -j CUSTOM-SERVICES-2\n</code></pre> <p>If you like, you can add a few more pods, and check the Iptables chains again, <code>iptables -t nat -L</code>. We can see how on a cluster with a few hundred endpoints, the rules would grow rapidly, becoming harder to follow. Also worth noting that with increasing number of rules does come some computing performance hit. Each chain that has to be checked is either pushed(--jump) or replacing(--goto) a frame on the stack and popped after completion.</p> <pre><code>sudo iptables -t nat -D CUSTOM-NODEPORTS -m comment --comment \"Nodepoort to service-1 pods\" -p tcp --dport 65509 -j CUSTOM-EXT-SERVICE-1\nsudo iptables -t nat -D CUSTOM-NODEPORTS -m comment --comment \"Nodepoort to service-2 pods\" -p tcp --dport 65510 -j CUSTOM-EXT-SERVICE-2\nsudo iptables -A CUSTOM-SEP-1 -t nat -j DNAT --to-destination 100.64.0.9:8080\nsudo iptables -A CUSTOM-SEP-2 -t nat -j DNAT --to-destination 100.64.0.10:8080\nsudo iptables -A CUSTOM-SEP-3 -t nat -p tcp -j DNAT --to-destination 100.64.0.19:8080\nsudo iptables -A CUSTOM-SEP-4 -t nat -p tcp -j DNAT --to-destination 100.64.0.20:8080\n</code></pre> <p>OPTIONAL: We can expose the nodePort-1 endpoint(s) created above behind an elastic loadbanalcer (ALB). - choose subnets with public network gateways for the ALB to allow connecting from a web browser</p> <pre><code>loadBalancerArn=$(aws ellbv2 create-load-balancer --name custom-alb-2 \\\n--subnets subnet-02468af75594a6335 subnet-025434f8fe8868f14 --security-groups sg-05884928af06d5957 | jq -rc '.LoadBalancer[].LoadBalancerArn')\n\ntargetGroupArn=$(aws elbv2 create-target-group --name service1-targets --protocol HTTP --port 65509 --health-check-port traffic-port \\\n--vpc-id vpc-0b50ec3f3729df7a8 --ip-address-type ipv4 --target-type instance | jq -rc '.TargetGroups[].TargetGroupArn')\n\naws elbv2 register-targets --target-group-arn $targetGroupArn --targets Id=`${node-EC2-ID:i-00817b787137b46c7}`\n\nlistenerArn=$(aws elbv2 create-listener --load-balancer-arn $loadbalancerarn --protocol HTTP --port 80  \\\n--default-actions \"Type=fixed-response\",\"FixedResponseConfig={MessageBody=Retry Later,StatusCode=503,ContentType=text/plain}\" | jq -rc '.Listeners.ListernerArn')\n\naws elbv2 create-rule --listener-arn $listenerArn --priority 1 \\\n--conditions \"Field=path-pattern\",\"PathPatternConfig={Values=[/,/svc1,/service1]}\" --actions \"Type=forward\",TargetGroupArn=$targetGroupArn\n</code></pre> <ul> <li>get the ALB DNS and paste it into a web browser, and note the non-uniform traffic distribution, even with 50% probability of hitting Pod-1 and Pod-3</li> </ul> <pre><code>aws elbv2 describe-load-balancers --name custom-alb --region us-west-2 | jq -rc '.LoadBalancers[].DNSName\n</code></pre>"},{"location":"article9-netwrk2/#-if-you-want-to-expose-nodeport-2-behind-the-same-alb-update-nginx-config-file-url-path-served-in-the-directory-runcontainerdpod-1rootfsetcnginxconfddefaultconf-to-serve-location-svc1-and-svc2-for-the-alb-to-route-to-but-this-is-not-of-interest-in-this-discussion","title":"- if you want to expose nodePort-2 behind the same ALB, update nginx config file url path served in the directory, <code>/run/containerd/pod-1/rootfs/etc/nginx/conf.d/default.conf</code> to serve location <code>/svc1</code> and <code>svc2</code> for the ALB to route to, but this is not of interest in this discussion.","text":""},{"location":"article9-netwrk2/#implementing-ipvs","title":"Implementing IPVS","text":"<p>With the growing chains and rules, and non-uniform traffic distribution with Iptables apparent, IPVS aims to solve these issues by enabling accurate algorithmic load distribution, and stream-lined rules management. In future EKS AMI release, required kernel modules may be enabled by default. For now let's check then install the required modules for IPVS;</p> <pre><code>sudo ipvsadm\n\n# if not present\nyum install ipset ipvsadm -y\nmodprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack\nsudo sysctl --write net.ipv4.vs.conntrack=1\n\nipvsadm --list -n\n</code></pre> <p>IPVS acts as a more accurate loadbalancer implementation over Iptables, in Linux kernel. It creates a virtual service (these are kubernetes virtual service IP), which fronts a cluster of real servers (in kubernetes, these are the pod endpoints behind the virtual service), and algorithmically loadbalances traffic to these service endpoints. There are a number of loadbalancing algorithms, available for configuration. However, for our use case, we use the round robin (ipvs_rr) algorithm. We are next going to create ipvs sets to replace our current custom iptables rules;</p> <ol> <li>we start by creating an IPVS service endpoint from the 198.19.0.0/16 service CIDR for service-1, 198.19.10.0/32 and service-2, 192.19.20.0/32. These are going to front the endpoints, pod-1, pod-3, pod-2, and pod-4, respectively, with a round_robin distributor algorithm. It is always a good idea to check the documentation of ipvs tool, <code>man ipvsadm</code>, note the packet-forwarding-method, and that it can only distribute tcp(6) and udp(17) traffic, so will again enforce the tcp as well as udp in the rules.</li> </ol> <p>|   a. create IPVS service implementation for the clusterIP service types discussed above;</p> <pre><code>sudo ipvsadm --add-service --tcp-service 198.19.10.0:8080 --scheduler rr\nsudo ipvsadm --add-service --tcp-service 198.19.20.0:8080 --scheduler rr\n</code></pre> <p>|   b. add the server endpoint, i.e, the pods, that the service routes to. Masquerading packet for our packet-forwarding-method;</p> <pre><code>sudo ipvsadm --add-server --tcp-service 198.19.10.0:8080 --real-server 100.64.0.9:8080 --masquerading\nsudo ipvsadm --add-server --tcp-service 198.19.10.0:8080 --real-server 100.64.0.19:8080 --masquerading\n\nsudo ipvsadm --add-server --tcp-service 198.19.20.0:8080 --real-server 100.64.0.10:8080 --masquerading\nsudo ipvsadm --add-server --tcp-service 198.19.20.0:8080 --real-server 100.64.0.20:8080 --masquerading\n</code></pre> <p>|   c. we test the traffic distribution to the service again, with the round robin algorithm, traffic is now uniformly distributed between both endpoints, using the IPVS virtual service. 5 connections should be distributed to pod-1 and pod-3 each, as well as for pod-2 and pod-4.</p> <p>NB: ensure that ipv4_forward is enabled, as mentioned in the ipvsadm man page - <code>sudo sysctl net.ipv4.ip_forward</code>. If 0, see last article to enable</p> <pre><code>for i in {1..10}; do curl -ikvs 198.19.10.0:8080/ 2&gt;/dev/null; done | grep \"Pod-3\" | wc -l\nfor i in {1..10}; do curl -ikvs 198.19.20.0:8080/ 2&gt;/dev/null; done | grep \"Pod-4\" | wc -l\n</code></pre> <p>This is all the configuration required to expose the endpoints behind virtual services, and have accurate loadbalancing implemented. However, notice that we still use iptables for pod-pod, pod-host, pod-external connections. You can check connections from one pod to the IPVS virtual service IPs using a tcpdump on the pod net-ns. We can confirm there is no destination and the packet from the pod is discarded.</p> <pre><code>sudo ip netns exec veth-ns-test-pod-1 curl -ikv 198.19.20.0:8080\nsudo ip netns exec veth-ns-test-pod-2 curl -ikv 198.19.10.0:8080\n</code></pre> <ol> <li>We are going to implement internal custom cluster network routes using IPVS next. Again, we simulate how kube-proxy would configure cluster IPVS. So we are going to create a dummy interface, custom-ipvs0 (kube-proxy sets up <code>kube-ipvs0</code>), and attach the IPVS service IPS to it. This is not dissimilar to how aws-node CNI sets up pod-to-pod networking using dummy interface, dummy0, for simply registering routable paths, as mentioned in the last article</li> </ol> <p>|   a. create a dummy interface for simple destination routing to any number of attached IPs/endpoints</p> <pre><code>sudo ip link add dev custom-ipvs0 type dummy\n\nsudo ip addr add 198.19.20.0/32 dev custom-ipvs0\nsudo ip addr add 198.19.10.0/32 dev custom-ipvs0\n</code></pre> <p>|   b. we start replacing the iptables rule by creating an ipset of set type; <code>hash:ip,port,ip</code>, same as kube-proxy, that can then be referenced in nft iptables to set entire block firewall rules, instead of the current individual rule for each clusterIp, nodePort, etc.  First, we allow loopback address from pod network namespaces to itself, for when we delete the custom-cni bridge device. Next, we configure similar rules to kube-proxy masquerading, in the nat-POSTROUTING chain for our custom IPVS chains;</p> <pre><code>sudo ipset create CUSTOM-LOOP-BACK hash:ip,port,ip\nsudo ipset add CUSTOM-LOOP-BACK 100.64.0.9,tcp:8080,100.64.0.9\nsudo ipset add CUSTOM-LOOP-BACK 100.64.0.10,tcp:8080,100.64.0.10\nsudo ipset add CUSTOM-LOOP-BACK 100.64.0.19,tcp:8080,100.64.0.19\nsudo ipset add CUSTOM-LOOP-BACK 100.64.0.20,tcp:8080,100.64.0.20\n\nsudo iptables -I POSTROUTING -t nat -m set --match-set CUSTOM-LOOP-BACK dst,dst,src -m comment --comment \"Rule matching set hash dst ip:port,source-ip for solving hairpin\" -j MASQUERADE\n</code></pre> <p>|   c. we replace the iptables rules for clusterIP with ipvs sets.</p> <pre><code>sudo ipset create CUSTOM-CLUSTER-IP hash:ip,port\nsudo ipset add CUSTOM-CLUSTER-IP 198.19.10.0,tcp:8080\nsudo ipset add CUSTOM-CLUSTER-IP 198.19.20.0,tcp:8080\n\nsudo iptables -A CUSTOM-CLUSTER-SERVICES -t nat -m comment --comment \"***\" -m set --match-set CUSTOM-CLUSTER-IP dst,dst -j ACCEPT\n</code></pre> <p>|   d. next, we create ipvs servers for nodePort service types, and map real servers to the destinations pods, then replace the iptables rules we creasted for nodePort with ipvs sets for our ipvs server.</p> <pre><code>nodeIP=$(sudo ip -details -j addr show | jq '.[].addr_info[] | select(.label == \"eth0\") | .local') &amp;&amp; localIP=$(sudo echo ${nodeIP} | tr -d '\"')\nsudo ipvsadm --add-service --tcp-service ${localIP}:65512 --scheduler rr\nsudo ipvsadm --add-service --tcp-service ${localIP}:65513 --scheduler rr\n\nsudo ipvsadm --add-server --tcp-service ${localIP}:65512 --real-server 100.64.0.9:8080 --masquerading\nsudo ipvsadm --add-server --tcp-service ${localIP}:65512 --real-server 100.64.0.19:8080 --masquerading\nsudo ipvsadm --add-server --tcp-service ${localIP}:65513 --real-server 100.64.0.10:8080 --masquerading\nsudo ipvsadm --add-server --tcp-service ${localIP}:65513 --real-server 100.64.0.20:8080 --masquerading\n\nsudo ipset create CUSTOM-NODE-PORT-TCP bitmap:port range 65500-65535\nsudo ipset add CUSTOM-NODE-PORT-TCP 65512\nsudo ipset add CUSTOM-NODE-PORT-TCP 65513\n\nsudo ipvsadm --add-service --tcp-service ${localIP}:65512 --scheduler rr\nsudo ipvsadm --add-service --tcp-service ${localIP}:65513 --scheduler rr\nsudo iptables -I CUSTOM-CLUSTER-SERVICES -t nat -m comment --comment \"***\" -m addrtype --dst-type LOCAL -j CUSTOM-NODEPORTS\nsudo iptables -I  CUSTOM-NODEPORTS -t nat -m comment --comment \"***\" -m set --match-set CUSTOM-NODE-PORT-TCP dst -j ACCEPT\n\nfor i in {1..10}; do curl -ikvs ${localIP}:65512 2&gt;/dev/null; done | grep \"Pod-3\" | wc -l\nfor i in {1..10}; do curl -ikvs ${localIP}:65513 2&gt;/dev/null; done | grep \"Pod-4\" | wc -l\n</code></pre> <p>And that is all the rules required to configure clusterIP and nodePort service types using ipvs. Compared with all the configuration required for using just iptables, this implementation requires drastically fewer rules.</p>"},{"location":"article9-netwrk2/#clean-up","title":"Clean-up","text":"<p>All the iptables rules can be deleted by providing the delete argument in all the shell commands used to configure the rule, like so;</p> <pre><code>sudo iptables -D CUSTOM-SEP-3 -t nat -p tcp -j DNAT --to-destination 100.64.0.19:8080\nsudo iptables -D CUSTOM-SEP-4 -t nat -p tcp -j DNAT --to-destination 100.64.0.20:8080\n\nsudo iptables -t nat -D CUSTOM-SERVICES-1 -m comment --comment \"Rule to service-1-endpoionts 50% Probability\" -m statistic --mode random --probability 0.5 -j CUSTOM-SEP-3\nsudo iptables -t nat -D CUSTOM-SERVICES-2 -m comment --comment \"Rule to service-2-endpoionts 50% Probability\" -m statistic --mode random --probability 0.5 -j CUSTOM-SEP-4\n\n...\n</code></pre>"}]}